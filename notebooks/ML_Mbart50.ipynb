{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:00.043235Z",
     "iopub.status.busy": "2025-03-01T07:17:00.043042Z",
     "iopub.status.idle": "2025-03-01T07:17:03.557902Z",
     "shell.execute_reply": "2025-03-01T07:17:03.556923Z",
     "shell.execute_reply.started": "2025-03-01T07:17:00.043215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:03.559257Z",
     "iopub.status.busy": "2025-03-01T07:17:03.558844Z",
     "iopub.status.idle": "2025-03-01T07:17:08.708870Z",
     "shell.execute_reply": "2025-03-01T07:17:08.708032Z",
     "shell.execute_reply.started": "2025-03-01T07:17:03.559228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q transformers sentencepiece datasets accelerate evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:08.710094Z",
     "iopub.status.busy": "2025-03-01T07:17:08.709836Z",
     "iopub.status.idle": "2025-03-01T07:17:12.370780Z",
     "shell.execute_reply": "2025-03-01T07:17:12.370164Z",
     "shell.execute_reply.started": "2025-03-01T07:17:08.710072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a09b044136d45f095e62067c7ed15b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65892e2b1dc940b382f640bd25171320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25520e1993584349a7a0487d9e9ec303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262361f34163423aa2dea9b9943283aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/133317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fe3f80098446a79f0a1a58eed7581f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a9f040fdc448f5a224111fb9f81541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"thainq107/iwslt2015-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:12.371950Z",
     "iopub.status.busy": "2025-03-01T07:17:12.371483Z",
     "iopub.status.idle": "2025-03-01T07:17:12.377012Z",
     "shell.execute_reply": "2025-03-01T07:17:12.376101Z",
     "shell.execute_reply.started": "2025-03-01T07:17:12.371926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 133317\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:12.378307Z",
     "iopub.status.busy": "2025-03-01T07:17:12.377990Z",
     "iopub.status.idle": "2025-03-01T07:17:12.398869Z",
     "shell.execute_reply": "2025-03-01T07:17:12.398223Z",
     "shell.execute_reply.started": "2025-03-01T07:17:12.378272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Rachel Pike : The science behind a climate headline',\n",
       " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:12.401357Z",
     "iopub.status.busy": "2025-03-01T07:17:12.401170Z",
     "iopub.status.idle": "2025-03-01T07:17:19.996031Z",
     "shell.execute_reply": "2025-03-01T07:17:19.995279Z",
     "shell.execute_reply.started": "2025-03-01T07:17:12.401341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be686112df884c0b836f51e94154cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dae35272d1c44058abc63e327b8685b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0fa98a51b14d3b83f1105b5e4e47ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac1abf8699c462d8d876ba551ff07ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecccb13e6dc244d3af84906ba9ec7899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:19.997838Z",
     "iopub.status.busy": "2025-03-01T07:17:19.997289Z",
     "iopub.status.idle": "2025-03-01T07:17:20.002992Z",
     "shell.execute_reply": "2025-03-01T07:17:20.002124Z",
     "shell.execute_reply.started": "2025-03-01T07:17:19.997804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:20.003896Z",
     "iopub.status.busy": "2025-03-01T07:17:20.003644Z",
     "iopub.status.idle": "2025-03-01T07:17:20.016274Z",
     "shell.execute_reply": "2025-03-01T07:17:20.015644Z",
     "shell.execute_reply.started": "2025-03-01T07:17:20.003875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    input_ids = tokenizer(\n",
    "        examples[\"en\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    labels = tokenizer(\n",
    "        examples[\"vi\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "\n",
    "    # Lấy ra ID của token pad\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels[labels == pad_token_id] = -100\n",
    "    return {\n",
    "        'input_ids' : input_ids, \n",
    "        'labels' : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:20.017632Z",
     "iopub.status.busy": "2025-03-01T07:17:20.017293Z",
     "iopub.status.idle": "2025-03-01T07:17:20.231987Z",
     "shell.execute_reply": "2025-03-01T07:17:20.231194Z",
     "shell.execute_reply.started": "2025-03-01T07:17:20.017602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045befb2cc474b788acbd44ffa27f700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34dbb18dbb74cc497dc47810f1a4cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9f6fe488294ee695ba6b95c68e169f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'vi', 'input_ids', 'labels'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = ds['train'].select(range(20)).map(preprocess_function, batched=True)\n",
    "ds_val = ds['validation'].select(range(20)).map(preprocess_function, batched=True)\n",
    "ds_test = ds['test'].select(range(10)).map(preprocess_function, batched=True)\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:20.233279Z",
     "iopub.status.busy": "2025-03-01T07:17:20.232975Z",
     "iopub.status.idle": "2025-03-01T07:17:48.205946Z",
     "shell.execute_reply": "2025-03-01T07:17:48.205039Z",
     "shell.execute_reply.started": "2025-03-01T07:17:20.233249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb325fd820644833971a3df21461d08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844a676146a345eabdbebe000dedf440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:48.207764Z",
     "iopub.status.busy": "2025-03-01T07:17:48.207007Z",
     "iopub.status.idle": "2025-03-01T07:17:48.215066Z",
     "shell.execute_reply": "2025-03-01T07:17:48.214182Z",
     "shell.execute_reply.started": "2025-03-01T07:17:48.207728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:48.216237Z",
     "iopub.status.busy": "2025-03-01T07:17:48.215943Z",
     "iopub.status.idle": "2025-03-01T07:17:59.087687Z",
     "shell.execute_reply": "2025-03-01T07:17:59.086858Z",
     "shell.execute_reply.started": "2025-03-01T07:17:48.216208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 250024, 127055,  66937,     13,     12,  67766,   2546, 218877,\n",
       "            858,    889,  10037,   6248,   1893,  17964,  42254,      2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_sample = torch.tensor(ds_train[0]['input_ids']).unsqueeze(0)\n",
    "labels_sample = torch.tensor(ds_train[0]['labels']).unsqueeze(0)\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "labels_sample[labels_sample == -100] = pad_token_id\n",
    "\n",
    "preds = model.generate(input_ids=preds_sample)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:59.088717Z",
     "iopub.status.busy": "2025-03-01T07:17:59.088499Z",
     "iopub.status.idle": "2025-03-01T07:17:59.094930Z",
     "shell.execute_reply": "2025-03-01T07:17:59.094157Z",
     "shell.execute_reply.started": "2025-03-01T07:17:59.088699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_pred ['Rachel Pike: Khoa học đằng sau một tiêu đề về khí hậu']\n",
      "decoded_label ['Khoa học đằng sau một tiêu đề về khí hậu']\n"
     ]
    }
   ],
   "source": [
    "decoded_pred = tokenizer.batch_decode(\n",
    "    preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "decoded_label = tokenizer.batch_decode(\n",
    "    labels_sample, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(\"decoded_pred\", decoded_pred)\n",
    "print(\"decoded_label\", decoded_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:59.096028Z",
     "iopub.status.busy": "2025-03-01T07:17:59.095742Z",
     "iopub.status.idle": "2025-03-01T07:17:59.877742Z",
     "shell.execute_reply": "2025-03-01T07:17:59.877078Z",
     "shell.execute_reply.started": "2025-03-01T07:17:59.096000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064e08b3f01b488b8de9c46910bb3b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Lấy ra ID của token pad\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = pad_token_id\n",
    "\n",
    "    decoded_pred = tokenizer.batch_decode(\n",
    "        preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    decoded_label = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    decoded_pred, decoded_label = postprocess_text(decoded_pred, decoded_label)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_pred,\n",
    "                            references=decoded_label)\n",
    "\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:17:59.879326Z",
     "iopub.status.busy": "2025-03-01T07:17:59.878584Z",
     "iopub.status.idle": "2025-03-01T07:20:14.281324Z",
     "shell.execute_reply": "2025-03-01T07:20:14.280609Z",
     "shell.execute_reply.started": "2025-03-01T07:17:59.879292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-15-4be71c190410>:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.010400</td>\n",
       "      <td>2.681265</td>\n",
       "      <td>4.091715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.926300</td>\n",
       "      <td>2.431512</td>\n",
       "      <td>2.903776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.481000</td>\n",
       "      <td>2.314733</td>\n",
       "      <td>8.130164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=2.4725913206736245, metrics={'train_runtime': 131.5272, 'train_samples_per_second': 0.456, 'train_steps_per_second': 0.023, 'total_flos': 9523519488000.0, 'train_loss': 2.4725913206736245, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable wandb\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/mBart50/en-vi-mbart50\",\n",
    "    predict_with_generate=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,  # You can also use 'processing_class=tokenizer' if needed\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences (beam-greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:20:14.282395Z",
     "iopub.status.busy": "2025-03-01T07:20:14.282146Z",
     "iopub.status.idle": "2025-03-01T07:20:24.486105Z",
     "shell.execute_reply": "2025-03-01T07:20:24.485362Z",
     "shell.execute_reply.started": "2025-03-01T07:20:14.282375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"/kaggle/working/mBart50/en-vi-mbart50\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/mBart50/en-vi-mbart50\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/mBart50/en-vi-mbart50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T07:20:24.487308Z",
     "iopub.status.busy": "2025-03-01T07:20:24.487014Z",
     "iopub.status.idle": "2025-03-01T07:20:30.297448Z",
     "shell.execute_reply": "2025-03-01T07:20:30.296738Z",
     "shell.execute_reply.started": "2025-03-01T07:20:24.487277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (greedy): BLEU = 0.48 11.8/1.2/0.3/0.2 (BP = 0.512 ratio = 0.599 hyp_len = 178 ref_len = 297)\n",
      "BLEU (beam): BLEU = 0.53 10.4/0.9/0.2/0.1 (BP = 0.713 ratio = 0.747 hyp_len = 222 ref_len = 297)\n"
     ]
    }
   ],
   "source": [
    "# Download model\n",
    "import sacrebleu\n",
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(task=\"translation_en_to_vi\", model=model, tokenizer = tokenizer)\n",
    "\n",
    "# Greedy search\n",
    "pred_sentences_greedy = translator(ds_test[\"en\"], batch_size=8, num_beams=1, do_sample=False)\n",
    "# Chuyển List[Dict[\"translation_text\"]] => List[str]\n",
    "pred_sentences_greedy = [item[\"translation_text\"] for item in pred_sentences_greedy]\n",
    "\n",
    "\n",
    "# Beam search\n",
    "pred_sentences_beam = translator(ds_test[\"en\"], batch_size=8, num_beams=5)\n",
    "# Chuyển List[Dict[\"translation_text\"]] => List[str]\n",
    "pred_sentences_beam = [item[\"translation_text\"] for item in pred_sentences_beam]\n",
    "# Tính BLEU cho từng kiểu suy luận\n",
    "bleu_score_greedy = sacrebleu.corpus_bleu(pred_sentences_greedy, [ds_test[\"vi\"]], force=True)\n",
    "bleu_score_beam = sacrebleu.corpus_bleu(pred_sentences_beam, [ds_test[\"vi\"]], force=True)\n",
    "\n",
    "print(\"BLEU (greedy):\", bleu_score_greedy)\n",
    "print(\"BLEU (beam):\", bleu_score_beam)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
